{"pages":[],"posts":[{"title":"DT35相关的备忘说明","text":"零点标定的初步方法 现在认为电压与距离的关系即D(mm)=0.000995*V(uV,直接的读取值)+50；那么在这个基础上，50mm内的测量肯定不可靠，DT35零位标定方法就初定为 均匀选取10个左右的距离值，D=0.000995*V+50+R，这些值与实际距离真值做差产生10个残差，求当残差的平方和有最小值时的R（求导数为零的情况） matlab实现如下，其中actual_data为距离真值，roughdeal_data为通过上述一元公式求得的，最后运行后打印出Xs0就是R的值，R即为尺子零点到DT35零点的距离（即DT35零点为尺子上刻度为R的地方）（小心方向） 1234567syms x %零点补偿量y=0;for i=1:length(actual_data) y=y+(actual_data(i)-roughdeal_data(i)-x)^2;enddy=diff(y,x);Xs0=solve(dy,x); 判断激光光线是否垂直车边的初步方法 利用约6m长的标定架，车固定在其中一端，测量激光发射中点到侧边标定架的距离；理论上激光应平行于标定架射向另一端，在另一端再次测量激光中点到侧边标定架的距离，两次测量应相等，目前规定差值不能大于1mm，至少测量三次，可能还需要换边测量 这样的方法只能让光线与由两个侧向导轮确定的直线平行,所以导轮的安装必须尽量规范，全场定位技术也需要这一点！！ 采集卡相关问题 在主控ucos操作系统上的速率和准确率测试需要完善 电压输入探针：现在通道1因为电阻焊盘焊掉失效了；已知通道1分压电阻阻值为M级，温漂百分级，可能需要换成千分级，已测，最大误差5mm以内；3、4通道也要分压电阻不过阻值过小，可能要重新焊M级电阻；其余通道都只是一个保护电阻","link":"/2020/03/12/DT35%E7%9B%B8%E5%85%B3%E7%9A%84%E5%A4%87%E5%BF%98%E8%AF%B4%E6%98%8E/"},{"title":"Ielts小班课学习-1","text":"雅思听力（今日重点：地图题） 常见单词 所在位置 形 状 交汇处 最高点 底 点 人行横道 entrance diamond(菱形) intersection top bottom pavement access oval(椭圆) junction peak base sidewalk entry gate square crossing summit foot pedestrian crossing lobby circle cross-road climax lowest point reception block rectangular cross-street crest information centre triangular corner car park 雅思口语（回顾第一题，发音练习） 口语表达短语 over the moon --> extremely happy/pleased an arm and a leg --> very expensiveeg. It cost me an arm and a leg,.... a blessing in disguise --> 因祸得福 (feel/be) down in the dumps --> sad the in thing --> something fashionable 雅思写作（大作文：报告类题目） 大作文所有题型 问法 利弊分析题 1.Do the advantages outweigh disadvantages? 2.Is this a positive or negative development? 是否同意 To what extent do you agree or disagree? 报告类题目 问题将在下面给出 双边讨论题 Discuss both views and give your own opinion. 报告类题目通过题目最后的两个问句(引起的问题?如何解决?)来判断，其次思考以下三个问题 What are the reasons?What can be done to solve it?What are the effects of it? 报告类题目作文结构： 开头段（可以较简洁）：背景句、改写句、观点句（例如：在下面的文章中将介绍问题和解决方法） 主体段（3段左右）：引起的问题（论点+解释论句–因果、对比、举例） 总结段：没有新增内容，关于总结的短语+观点+回答 报告类大作文结构1 报告类大作文结构2 开头段 开头段 原因1+论证 原因1 原因2+论证 解决方案1 （原因3+论证） 原因2 解决方案1+论证 解决方案2 解决方案2+论证 原因3 （解决方案3+论证） 解决方案3 结尾段 结尾段 以下为一篇例文中的结构 In the following essay,I will discuss some of the problems this causes and also offer some solutions. One of the main problems ... Perhaps the most significant problems ... I believe the solution to these problems ... First .. Secondly,and perhaps more importantly ... consequenly ... In conclusion ... 生词记忆 单词 释义 单词 释义 jog 慢跑 corridor 房间走廊 auditorium 礼堂、观众席 marsh 沼泽、湿地 stables 牲畜棚 row of buliding 一排建筑 confidentiality 机密 naval 海军的 bump into 撞见 issue 发行，分发 back then 在那时 haze 雾霾 respect 方面 cosmetic 化妆品的 water sprinkling 泼水节 lantern festival 元宵节 dragon boat fastival 端午节","link":"/2020/03/14/2020-3-14/"},{"title":"Ielts小班课学习-4","text":"雅思口语词汇 单词 释义 单词 释义 burmese 缅甸人 burma 缅甸 portuguese 葡萄牙人 portugal 葡萄牙 iraqi 伊拉克人 kuwaiti 科威特人 cab 出租车 distractor 干扰项 northern 注意拼写 southern 注意拼写 hostel 小旅馆 prompts 提示 introverted 内向的 arduous 非常困难的 part two 口语表达积累 I’d like to talk about ….&emsp;&emsp;开头 to be honest, this question really got me at the beginning (because ….) to tell the truth ,a (little) bit the idea come to my mind why not …. which …. specialised for people who … you know/see so in this case/in that way , .. be able to&emsp;&emsp;直接替换can，不要用can了 it would be a relief for people have cool feet when&emsp;&emsp;害怕 雅思小作文 在小作文地比较中，表示倍数可以使用times、fold(like eightfold)，表示百分比有percentage、proportion，表示“占比”的动词有account for、make up、represent 那么占比的三种表达方式，不要细究句意与难度，记住形式 1231. the percent of excellent evaluation climb to 28%2. The excellent part accounts for 28 percent of the questionnaire3. 28% customer service evaluation is excellent 单词 释义 单词 释义 subsequently 随后（放句首） once-in-a-lifetime 一生一次的 Conversely 相反地 雅思阅读 作业","link":"/2020/04/04/Ielts-4/"},{"title":"Ielts小班课学习-3","text":"雅思口语 口语中如何表达喜欢 I come to like…&emsp;表示渐进的过程 sth. is my cup of tea sth. takes my fancy most I’m quite fond of… I’m really into… I’m quite/really keen on… I’m a big/massive fan of… 口语表达积累 just take myself as an example honestly, i don’t really know, guess might just be from time to time change my sedentary lifestyle&emsp;改变久坐的生活方式 there might be… perhaps , i would/probably say… on top of that&emsp;除此之外 to tell you the truth harness the power of the wind/swift current&emsp;借助风/湍流的力量。。 can’t wait to give it a try hair blowing in the wind&emsp;微风轻拂过你的头发 i don’t know. if you’re talking about xxx i have got used to&emsp;我已经习惯于 it may be more likely for me to…&emsp;更可能 after my retirement&emsp;退休之后 how fantastic! gentle breeze from the sea&emsp;轻柔的海风 bask in the morning sun&emsp;晒晒早上的太阳 It kind of depends on&emsp;一定程度上取决于。。 口语词汇 单词 释义 单词 释义 soothing a. 舒缓的，使人宽心的 therapy n. 治疗，疗法 medium n. 媒介；方法 intensive a. 高强度的 fly-boarding 水上飞行 water skiing 滑水运动 breathtaking a. 惊艳的 goggles n. 护目镜 well-liked a. 受喜爱的 coastal city 沿海城市 sports fanatics 体育爱好者 breaststroke 蛙泳 表示非常的副词：pretty、quite、extremely、comparatively 雅思作文“利弊分析题”大作文结构 段落 五五开式 有偏向式1 有偏向式1 1 开头段 开头段 开头段 （既有好处又有坏处） 一个观点 一个观点 2 好处1+论证 我方观点1 让步段 好处2+论证 理由+论证 说明对方观点+论证 3 坏处1+论证 我方观点2 我方观点1 坏处2+论证 理由+论证 理由+论证 4 结尾段 让步段 我方观点2 说明对方观点+论证 理由+论证 5 结尾段 结尾段 表转折的连接词：whereas、while、but、however、nevertheless、 小作文中表程度的词：severely、dramatically、rapidly、significantly、sharply、considerately;slightly、slowly;steadily、gently、moderately 小作文中“超过”含义：exceed、overtake 小作文结尾，常用Overall:开头 作文词汇 单词 释义 单词 释义 cultivate v. 栽培；耕种 fluctuate v. 波动，涨落 fluctuation n. 波动，起伏 respectively adv. 各自地 approximately adv. 近似地，大约 much less significantly 相较而言不重要的 短语 释义 短语 释义 all-round development 全面发展 cultivate the comprehensive quality 培养综合素质 professional knowledge 专业知识 compulsory curriculum/education 必修课程/教育","link":"/2020/03/28/Ielts%E5%B0%8F%E7%8F%AD%E8%AF%BE%E5%AD%A6%E4%B9%A0-3/"},{"title":"Ielts学习-2","text":"雅思听力 完成剑雅3个section，正确率正常不过用时较长，进入状态太慢 以下是学术背景的听力相关词汇积累，雅思喵807听力词汇 A B C D orientation resume curriculum curriculum vitae extra-curriculum assessment virtual compulsory trimester semester term session obligatory 雅思作文 思维发散小技巧 s-e-c-r-e-t of cheese 知识的秘密 skilleducationcarrer,communication,comments,curriculumread,reveal,review,record,refreshelevate,estimatetell,teach,threshold,translate,target,topic, Ielts作文评分标准 任务完成度 连贯与衔接 词汇资源 语法结构 TA CC LR GRA Task Achievement Coherence and Cohesion Lexical Resource Grammatical Range Accuracy 其他扣分点 underlength 篇幅不够 no of words 字数 off-topic 偏题 memorised 套用模板 illegible 字迹潦草 penalty 作弊 小作文 动态图（依据年份）、静态图 线（string）、柱（bar）、饼（pie）、表（form）、流程（flow）、地图（map） 图表 上升 下降 维持不变 diagram rise decrease stay steady chart rocket go down remain constant graph soar decline keep stable surge drop maintain this level climb fall go up diminish increase boom (a(an) upward/downward trend:一个上升下降趋势) 雅思阅读 平行阅读法：读完一遍文章后，全部题目做完，即按顺序阅读文章包括题目 判断题中的NG选项出现形势：1.完全没提及 2.提到一部分，另一部分没提及 作业：剑雅10 Test 3 -Passage 2 生词记忆 生词 释义 生词 释义 proofread v. 校对 punctuation n. 标点符号 preposition n. 介词 carnival n. 狂欢节，嘉年华 infinitive n. （动词的）不定式 reinvent v. 以新形式出现 pigment n. 色素，颜料 illegible a. 难以辨认的 penalty n. 惩罚","link":"/2020/03/21/Ielts-2/"},{"title":"Python!","text":"python学习目前进度，廖雪峰的教程 本来创建博客的想法是想记录学习python的过程，结果一个hexo博客花了我四五天的时间，不过还好搞出来了，有点小成就感，好了现在继续学习Python！！gogogo 以下是最新写的一个非常简单的成绩查询小程序，注意，有失败的地方已经标出 1234567891011121314151617181920212223242526272829303132333435363738394041#2020.3.9#学习python第二个小程序#计算今年成绩较去年的变化n=1m=1while n==1: #输入若干信息 name=input('请输入学生姓名：');print('\\n') s1=input('请输入%s去年的成绩：'%name) while m==1:#这样的方法失败了，目前无法解决成绩输入不是数字的重复正确输入问题 if type(s1)==float or type(s1)==int: if s1&gt;=0: m=0 s2=float(input('请输入%s今年的成绩：'%name));print('\\n') else: s1=input('输入失败，请正确输入%s去年的成绩：'%name) else: s1=input('输入失败，请正确输入%s去年的成绩：'%name) else: #判断成绩走向并计算变化率 if s2&gt;s1: rate=(s2-s1)/s2*100 print('%s同学你好，祝贺你进步了,成绩增长率为 %.2f%%\\n'%(name,rate)) elif s1==s2: print('%s同学你好，不错，成绩保持住了\\n'%name) rate=0.00 else: rate=(s1-s2)/s2*100 print('%s同学你好，很遗憾你退步了，成绩下降率为 %.2f%%\\n'%(name,rate)) n=int(input('如果希望继续查询，请输入1，否则输入其他任意键退出'))else: #结尾 print('欢迎再次使用,再见')","link":"/2020/03/12/Python/"},{"title":"pillow初识、format格式化、绝对相对路径说明","text":"python -m -pip install –upgrade pip #升级python的pip pip install pillow #安装pillow库 pip install django-simple-captcha #安装captcha库 pip install opencv-python #安装opencv库 pip install matplotlib #安装matplotlib库 pillow的小程度基本入门 可以对图像进行简单的读取、生成、裁剪、缩放、复制、翻转、旋转、粘贴、拉伸 图像的模糊、边缘增强、锐利、平滑等常见操作 在图像中加入文本，将图片变色 文件绝对路径和相对路径的表示形式不同 windows系统中绝对路径用\\展开，而在其他系统中会出现用/的展开 相对路径中则使用/，比如返回上一级文件夹../，当前文件夹前不需要加斜杠，前往下一级文件夹的某文件则类似于/xx/xx.c python语法：格式化输出之format用法","link":"/2020/03/27/PYTHON-%E4%B9%8B-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"},{"title":"adams仿真自学","text":"HITCRT分享adams仿真教程（目前删除） 初步仿真步骤 新建一个模型，设置各种单位规范 导入solidworks中生成的模型.x_t类型文件 建立约束，注意方向（包括固定副、旋转副但不限于此）（可依此设置机构初始状态） 建立驱动（驱动方程可联系至matlab描述，遵循教程） 建立力的接触（设置两实体的碰撞、摩擦等参数）Adams材料碰撞参数(excel) 开始仿真，步数（steps）越大计算时间越久，计算结果越准确 设置测量量，以图表形式显示","link":"/2020/03/21/adams/"},{"title":"Nice to meet you,Markdown","text":"markdown语法的主要说明 如何加入不同字长的空格 如何改变字体颜色大小等 字体颜色大全 如何插入文档下载链接 如何插入本地视频 进行页内跳转、建立目录","link":"/2020/03/10/markdown/"},{"title":"Hexo Learning Record","text":"目录索引2020/3/09 利用hexo在github上创建个人博客2020/3/10 更换hexo博客主题为icarus2020/3/11 开始学习修改icarus主题配置文件2020/3/12 使用valine开启评论功能 2020/3/09 利用hexo在github上创建个人博客 一直想有一个私人的空间记录自己不管是日常生活还是各种知识学习，b站真是个好东西，30分钟自主搭建成功自己的小网页，以后可以好好耍一耍了 此处附上hexo入门视频，感谢网络 2020/3/10 更换hexo博客主题为icarus 缺少cheerio文件或库 首先更换主题完后进行hexo指令发现直接出现错误，以为是download&nbsp; zip直接下载剪切进去太粗暴，于是又在github上直接clone到icarus文件夹中，问题还有，定睛一看错误的具体内容，有个叫cheerio的东西没安装，百度咋办，安装呗 npm install cheerio 无法部署到远程github网站 按好了之后终于可以套在自己的文章上了（虽然啥都没写），localhost/4000上预览之后准备推到github网站上，结果出了第二个问题，无法deploy到git，爷气死了，这又是啥玩意啊，不过还好百度找到了答案 hexo更新到3.0之后，deploy的type的github需要改成gitnpm install hexo-deployer-git --save&emsp;改了之后执行，然后再部署试试 这是参考解答链接&emsp;另一个参考 好了，可以开始学习icarus主题上的设置更改和优化了 2020/3/11 开始学习修改icarus主题配置文件 首先，放上github源（含部分介绍），本次只是记录部分不能理解或者不会使用的功能以及些许理解 分享网上的详细讲解，就不多说了 非常详细，黄小邪 2020/3/12 使用valine开启评论功能 这里是valine的详细获取APP id与APP key的说明","link":"/2020/03/09/hexo/"},{"title":"maxwell磁场仿真记录","text":"这里是ansys electronic 18安装地址 可用 提取码：se7i这里是详细安装、破解教程，可用 2020/3/11 这两天又开始和李忠港学长搞磁场仿真了，哎，说到这个专业，我也不知道我到底感不感兴趣。 府学长之前的文档里说0.4的规律（圆形线圈内径与外径之比）自己做仿真大概确实了这一点，仿的是矩形线圈，我个人感觉矩形线圈的对应系数可能略小于0.4，具体没有确定，如果以后自己做项目，追求目标比较极致，可能会需要确定 2020/3/12副边的旋转看样子对k的影响不大。不过对于一对矩形线圈，0°和90°的k值竟然不一样，难道是加入电流的边不对应电流有相位差？（瞎想，估计不是）","link":"/2020/03/11/maxwell%E7%A3%81%E5%9C%BA%E4%BB%BF%E7%9C%9F/"},{"title":"Nice to meet you,Hexo","text":"basic instruction hexo init&emsp;&nbsp;&thinsp;–&gt;初始化一个新博客hexo n&emsp;&emsp;–&gt;新建一个博客文档hexo s&emsp;&emsp;&thinsp;–&gt;加载本地博客服务器 hexo clean&ensp;–&gt;清理数据库hexo g&emsp;&emsp;–&gt;生成、更新博客 hexo d&emsp;&emsp;–&gt;部署到远端，即github下的html网站 update flow hexo clean –&gt; hexo g –&gt; hexo s –&gt; hexo d use hexo theme from github: git clone https://github.com/xxx.git theme/xxx _config.yml 中替换theme为新文件夹名称 hexo clean –&gt; hexo g –&gt; hexo s –&gt; hexo d","link":"/2020/03/09/nice%20to%20meet%20hexo/"},{"title":"python 之 opencv 阶段总结篇","text":"前言 没想到吧，这就总结了。没错我学习的三分钟热度真的太强了，这几天作业都没好好做，全在看这一套opencv的教程。那么就在大概第四天的时候把这一套教程给全部实验了一遍。 不管怎么说，这一套教程呢，我作为小白学者觉得非常不错，讲的不深也有一些小挑战（略微简单，不过需要细品）。其实可以通过这些知识和技巧来做自己的延申，我还没有多尝试，唯一一次尝试使用对比字母轮廓形状的方法找到正确的字母，用他给的‘A’成功了，自己在网上找了个‘C’，预处理还做了闭运算轮廓只有一条，结果算法结果是像A我真的吐了。 那么鉴于我一下子就学完了，而且每一章都像之前那样复制性的总结其实没多大意义，教程我电脑里也有，想复习了再去看看就可，所以这次来一个总结，目的就是略微的提到写我觉得的重点。 知识点回忆 1. 摄像头 or 视频2. matplotlib的使用3. 图像基本操作4. 颜色空间转换5. 阈值分割6. 图形几何变换7. 绘图功能8. 图像混合 1. 摄像头 or 视频 我看了下，之前讲完了前三个部分，调用电脑摄像头，可以用来截取一帧图像、可以将摄像头的连续若干帧图像保存成视频的形式 可以打开一个视频，如果你利用cv2来创建一个滑动条cv2.createTrackbar()（滑动就会对应有回调函数）还能实现拖动视频播放进度，借助cv2.setTrackbarPos()、cv2.getTrackbarPos()其实看函数名也知道是干什么的，这里不多讲了 cv2.VideoCapture()可以打开摄像头或者视频文件，cap.get(propId)、cap.set(propId,value)来得到和修改摄像头的很多属性 cv2.imwrite()来保存图片或者视频，保存视频的时候要注意提前设置好视频编码方式，我看有挺多的可能根据需要到时候就会了解吧、当然读取图片还是得cv2.imread() 捕获视频中的一帧ret, frame = capture.read() 2. matplotlib的使用 其实这整套教程也没咋用到，他和matlab里的plot用法有点像，都是subplot。不过要注意，他来显示图像时，plt.imshow()中后一个参数cmap的参数选择，灰度图就是cmap = 'gray'，而且imshow之后必须要加plt.show()，要不然图片就不显示，可以看一下老师怎么写的 1234567891011titles = ['Original', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV']images = [img, th1, th2, th3, th4, th5]# 使用Matplotlib显示for i in range(6): plt.subplot(2, 3, i + 1) plt.imshow(images[i], 'gray') plt.title(titles[i], fontsize=8) plt.xticks([]), plt.yticks([]) # 隐藏坐标轴plt.show() matplotlib中图像显示RGB顺序，opencv是BGR，显示时需要通道倒置一下 3. 图像基本操作 读取像素点的值直接img[100, 90]，修改也是这样。 img.shape形状中包括行数、列数和通道数 截取ROI，非常简单一看就懂，face = img[100:200, 115:188]，这也只能是个矩形，不过得到轮廓或者利用掩码方式得到一块想要的区域也行 彩色图的BGR三个通道是可以分开单独访问的，b, g, r = cv2.split(img)；也可以合并组成图像img = cv2.merge((b, g, r)) 4. 颜色空间转换 img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)图像形式转换，教程中用他来转换成HSV颜色模型，改成cv2.COLOR_BGR2HSV就行了 mask = cv2.inRange(hsv, lower_blue, upper_blue)，其实inRange这个函数我感觉不太重要，因为后面有阈值分割，但可能这个是针对真彩图像，与阈值分割多是用于灰度图 然后很重要的一个res = cv2.bitwise_and(frame, frame, mask=mask)，这个位操作是图像只存在0和255也就是二值图像的关键操作，可以在以后的二值图像处理里起到作用，特别是应用在掩码图的得到 5. 阈值分割 这一段是之后的基础，全都讲起来有点麻烦了，我就不多说。额，先说一句，这个应该是用在处理灰度图上的 固定阈值：ret, th = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)，第二个参数是设定的阈值，第三个是最大值，最后一个阈值的方式，种类主要5种 自适应阈值：th2 = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 4)，不同之处就在第三个参数，小区域阈值的计算方式；最后一个参数：最终阈值等于小区域计算出的阈值再减去此值（光看看不懂，想试试就自己试试） Otsu阈值：看代码就知道怎么用了，原理自便 123456789# 固定阈值法ret1, th1 = cv2.threshold(img, 100, 255, cv2.THRESH_BINARY)# Otsu阈值法ret2, th2 = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)# 先进行高斯滤波，再使用Otsu阈值法blur = cv2.GaussianBlur(img, (5, 5), 0)ret3, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) 光线不均匀的图片就用自适应阈值就完事了 6. 图形几何变换 其实我觉得这一章是很重要的，因为图像处理应该会有很多时候待处理的目标是需要几何变换特别是立体上的仿射变换 cv2.resize()可以自己定义大小，也可以按比例缩放，用处会很多，我不知道和下面这个有什么关系，用处大不大，或者说在哪里使用 12lower = cv2.pyrDown(img) # 向下采样一级higher = cv2.pyrUp(img) # 向上采样一级 cv2.flip(img, 1)第二个参数决定是镜像、垂直、镜像垂直翻转 平移图片，需要用到矩阵，用仿射变换函数cv2.warpAffine()实现；旋转也是，而且用这个函数前，还要加个得到旋转对应的矩阵的函数cv2.getRotationMatrix2D()，你看这还只是2D的 $$M = \\left[ \\begin{matrix} 1 &amp; 0 &amp; t_x \\newline 0 &amp; 1 &amp; t_y \\end{matrix} \\right]$$ 仿射变换 二维的，懂了吗，基本的图像变换就是二维坐标的变换：从一种二维坐标(x,y)到另一种二维坐标(u,v)的线性变换，你得先得到一个矩阵，那么就可以转换了，矩阵T(2×3)就称为仿射变换的变换矩阵 $$\\begin{matrix} u=a_1x+b_1y+c_1 \\newline v=a_2x+b_2y+c_2 \\end{matrix}$$ $$\\left[ \\begin{matrix} u \\newline v \\end{matrix} \\right] = \\left[ \\begin{matrix} a_1 &amp; b_1 \\newline a_2 &amp; b_2 \\end{matrix} \\right] \\left[ \\begin{matrix} x \\newline y \\end{matrix} \\right]+\\left[ \\begin{matrix} c_1 \\newline c_2 \\end{matrix} \\right]$$ $$R=\\left[ \\begin{matrix} a_1 &amp; b_1 \\newline a_2 &amp; b_2 \\end{matrix} \\right], t=\\left[ \\begin{matrix} c_1 \\newline c_2 \\end{matrix} \\right],T=\\left[ \\begin{matrix} R &amp; t \\end{matrix} \\right]$$ 然后几行代码看懂，现根据三个点和对应的三个点得到变换矩阵，然后直接转整个图像 12345678# 变换前的三个点pts1 = np.float32([[50, 65], [150, 65], [210, 210]])# 变换后的三个点pts2 = np.float32([[50, 100], [150, 65], [100, 250]])# 生成变换矩阵M = cv2.getAffineTransform(pts1, pts2)dst = cv2.warpAffine(img, M, (cols, rows)) 透视变换 三维的，就是立体上如何把一个目标看得顺眼一点，一个身份证你斜着拍的，那我想让他能像平着拍的一样舒服，那就用这个方法 $$\\begin{matrix} X=a_1 x + b_1 y + c_1 \\newline Y=a_2 x + b_2 y + c_2 \\newline Z=a_3 x + b_3 y + c_3 \\end{matrix}$$ $$\\left[ \\begin{matrix} X \\newline Y \\newline Z \\end{matrix} \\right] = \\left[ \\begin{matrix} a_1 &amp; b_1 &amp; c_1 \\newline a_2 &amp; b_2 &amp; c_2 \\newline a_3 &amp; b_3 &amp; c_3 \\end{matrix} \\right] \\left[ \\begin{matrix} x \\newline y \\newline 1 \\end{matrix} \\right]$$ OpenCV中首先根据变换前后的四个点用cv2.getPerspectiveTransform()生成3×3的变换矩阵，然后再用cv2.warpPerspective()进行透视变换 123456789# 原图中卡片的四个角点pts1 = np.float32([[148, 80], [437, 114], [94, 247], [423, 288]])# 变换后分别在左上、右上、左下、右下四个点pts2 = np.float32([[0, 0], [320, 0], [0, 178], [320, 178]])# 生成透视变换矩阵M = cv2.getPerspectiveTransform(pts1, pts2)# 进行透视变换，参数3是目标图像大小dst = cv2.warpPerspective(img, M, (320, 178)) 变换 矩阵 自由度 保持性质 平移 [I, t]（2×3） 2 方向/长度/夹角/平行性/直线性 刚体 [R, t]（2×3） 3 长度/夹角/平行性/直线性 相似 [sR, t]（2×3） 4 夹角/平行性/直线性 仿射 [T]（2×3） 6 平行性/直线性 透视 [T]（3×3） 8 直线性 7. 绘图功能 感觉画图功能不太用得到，不过也是用起来很方便，绘制形状的函数有一些共同的参数 color：绘制的颜色,是彩色图就估计是BGR，就输一个三维元组ok；灰度图就一个255以内的数 thickness：线宽，默认为1；对于矩形/圆之类的封闭形状而言，传入-1表示填充形状 建议画什么线型都加一个lineType=cv2.LINE_AA比较好看 img = np.zeros((512, 512, 3), np.uint8)可以直接用这样一句话来创建一个纯色图 cv2.line(img, (0, 0), (512, 512), (255, 0, 0), 5)，起点、终点 cv2.rectangle(img, (384, 0), (510, 128), (0, 255, 0), 3)，左上、右下 cv2.circle(img, (447, 63), 63, (0, 0, 255), -1)，圆心、半径 cv2.ellipse(img, (256, 256), (100, 50), 0, 0, 180, (255, 0, 0), -1)，椭圆中心、x/y轴长度、旋转角度、起始角度、结束角度，具体自己试试 12345# 使用cv2.polylines()画多条直线line1 = np.array([[100, 20], [300, 20]], np.int32).reshape((-1, 1, 2))line2 = np.array([[100, 60], [300, 60]], np.int32).reshape((-1, 1, 2))line3 = np.array([[100, 100], [300, 100]], np.int32).reshape((-1, 1, 2))cv2.polylines(img, [line1, line2, line3], True, (0, 255, 255)) 添加文字：cv2.putText(img, 'ex2tron', (10, 500), font,4, (255, 255, 255), 2, lineType=cv2.LINE_AA) 8. 图像混合 👴乏了，找时间再写吧","link":"/2020/04/03/opencv-4/"},{"title":"python 之 opencv-摄像头采集和视频播放","text":"打开摄像头 要使用摄像头，需要使用cv2.VideoCapture(0)创建VideoCapture对象，参数0指的是摄像头的编号，如果你电脑上有两个摄像头的话，访问第2个摄像头就可以传入1，依此类推。 capture.read函数返回的第1个参数ret(return value缩写)是一个布尔值，表示当前这一帧是否获取正确 cv2.cvtColor用来转换颜色，这里将彩色图转成灰度图 cap.get(propId)、cap.set(propId,value)通过cap.get(propId)可以获取摄像头的一些属性，比如捕获的分辨率，亮度和对比度等。propId是从0~18的数字，代表不同的属性，完整的属性列表可以参考：VideoCaptureProperties。也可以使用cap.set(propId,value)来修改属性值 经验之谈：某些摄像头设定分辨率等参数时会无效，因为它有固定的分辨率大小支持，一般可在摄像头的资料页中找到。 播放本地视频 跟打开摄像头一样，如果把摄像头的编号换成视频的路径就可以播放本地视频了。回想一下cv2.waitKey()，它的参数表示暂停时间，所以这个值越大，视频播放速度越慢，反之，播放速度越快，通常设置为25或30。 12345678910# 播放本地视频capture = cv2.VideoCapture('demo_video.mp4')while(capture.isOpened()): ret, frame = capture.read() gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) cv2.imshow('frame', gray) if cv2.waitKey(30) == ord('q'): break 录制视频 之前我们保存图片用的是cv2.imwrite()，要保存视频，我们需要创建一个VideoWriter的对象，需要给它传入四个参数： 输出的文件名，如’output.avi’ 编码方式FourCC码 帧率FPS 要保存的分辨率大小 FourCC是用来指定视频编码方式的四字节码，所有的编码可参考Video Codecs。如MJPG编码可以这样写： cv2.VideoWriter_fourcc(*'MJPG')或cv2.VideoWriter_fourcc('M','J','P','G') 12345678910111213141516capture = cv2.VideoCapture(0)# 定义编码方式并创建VideoWriter对象fourcc = cv2.VideoWriter_fourcc(*'MJPG')outfile = cv2.VideoWriter('output.avi', fourcc, 25., (640, 480))while(capture.isOpened()): ret, frame = capture.read() if ret: outfile.write(frame) # 写入文件 cv2.imshow('frame', frame) if cv2.waitKey(1) == ord('q'): break else: break OpenCV VideoCapture.get()参数中文详解，不太准确供参考 练习 实现一个可以拖动滑块播放视频的功能。（提示：需要用到 cv2.CAP_PROP_FRAME_COUNT和cv2.CAP_PROP_POS_FRAMES两个属性） 123456789101112131415161718192021222324252627282930import cv2def changetime(x): return capture.set(cv2.CAP_PROP_POS_FRAMES, x)# 播放本地视频capture = cv2.VideoCapture('demo_video.mp4')#获取视频帧长度frame_count = capture.get(cv2.CAP_PROP_FRAME_COUNT)#创建一个窗口cv2.namedWindow('video')# 创建视频滑动条cv2.createTrackbar('video', 'video', 0, int(frame_count), changetime)height = capture.get(cv2.CAP_PROP_FRAME_HEIGHT)width = capture.get(cv2.CAP_PROP_FRAME_WIDTH)print(capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height * 2))print(capture.set(cv2.CAP_PROP_FRAME_WIDTH, width * 2))while(capture.isOpened()): ret, frame = capture.read() if ret == True: cv2.setTrackbarPos('video','video',int(capture.get(cv2.CAP_PROP_POS_FRAMES))) cv2.imshow('video', frame) if cv2.waitKey(30) == ord('q'): break else: break","link":"/2020/03/30/opencv3/"},{"title":"a","text":"今天是在家的第70天，这70天里我还是出去了一次，去拿了个陀螺仪，一个两万多可真他妈贵，收据和发票一定别弄丢了，👴赔不起 我觉得以后还是一定要对朋友关心一些，如果我看不起任何人那我也就没有朋友，到生日了一定要送上祝福，最好能买礼物，所以平时也别太野性消费了。奶茶能不买就别喝了，好好在学校吃饭，攒钱在更有意义的地方 rc比赛估计是至少延期了，这比赛我其实很没信心的，本来我就不是个自信的人，队长天天说我们菜，越南不知道哪儿的学校的设计就能把我们搞的一愣一愣的，也许是我太没自信了吧","link":"/2020/03/30/a/"},{"title":"Ielts小班课学习-5","text":"雅思口语 回答part1、2时尤其是part1回答的内容尽量展开，不需要死揪住一个问题想回答他，而是可以先泛泛的回答，具体回答可以不用那么详细 单词 释义 单词 释义 three kingdoms period 三国时期 QR code(quick responds) 二维码 turn to sb for help 寻求帮助 tricky problem 麻烦的问题 口语表达积累 be used to … &emsp;&emsp;过去常常 It’s no big deal. &emsp;&emsp;这没什么大不了的 well, let me see ..&emsp;&emsp;（用在开头思考时避免冷场） i haven’t thought about that. but if you ask me ,i would say that i … this question really got me at the beginning it’s quiet different .. &emsp;&emsp;（想起来这种表达） have made lots of contributions to (have done) &emsp;&emsp;做过很多帮助、成就（时态加分） i can give a relevant example here &emsp;&emsp;给一个类似的例子 take sth. as an example fall out of my chair &emsp;&emsp;形容笑得不行 are all my first choice &amp; on my top of list &emsp;&emsp;表达都是我的最爱 do v. &emsp;&emsp;注意口语中通过这种方式表强调 as for … &amp; moving on to … &emsp;&emsp;内容的转换如何连接 back then … &emsp;&emsp;那时候 雅思小作文&emsp;&emsp;小作文两种文章结构都可以，结尾写在前面有助于可能没有时间写完的时候，由于结尾部分影响评分，所以写在前面 开头+主体+结尾 开头+结尾+主体 在遇到table图表类的小作文信息题时，要注意找到重点，就是该按什么样的顺序写。存在多个可分类同等信息列时（比如同时存在数个国家，数个资源的动态图），要分析是按国家一个一个写，还是一个一个资源依次写，防止过多的重复和冗余。 单词 释义 单词 释义 substantial 相当程度的;牢固的 fundamental 十分重大的;根本的;基础的 marginal 小的;微不足道的;不重要的 outnumber v. (在数量上)压倒，比…多 yield v. 出产;产生 n. 产量;产出 industrial a. 勤奋的(diligent) inherit v. 继承 雅思阅读 作业 4.09 生词积累 生词 释义 生词 释义 linguistic a.语言的,语言学的 n.语言学 deteriorate v.变坏,恶化,退化 invective n.咒骂;痛骂 polemic n.争辩,辩论 exempt a.被免除的,被豁免的 aptitude n.天资,天赋,才能 inherently a.内在的;固有的 propound v.提出(观点)供考虑 adherent n.(政党、思想的)拥护者，追随者，信徒 deviation n.偏常;反常;离经叛道;(统计学中的)离差，偏差 authoritarian a.专制的;独裁的 prescribe v.开(药、处方);(人、法律、规则)规定，指定 tenet n. (理论、信仰的)基本原则，根本信条 quasi-political a.准政治的 elitist a.精英主义的;精英论的 n.精英主义者;精英论者;精英","link":"/2020/04/13/ielts-5/"},{"title":"python 之 opencv-matplotlib显示图片","text":"Matplotlib Matplotlib是Python的一个很常用的绘图库，有兴趣的可以去官网学习更多内容。 显示灰度图12345678import cv2import matplotlib.pyplot as pltimg = cv2.imread('lena.jpg', 0)# 灰度图显示，cmap(color map)设置为grayplt.imshow(img, cmap='gray')plt.show() 结果如下： plt.show()和plt.imshow()的区别 plt.imshow 的cmap颜色参数选择 显示彩色图OpenCV中的图像是以BGR的通道顺序存储的，但Matplotlib是以RGB模式显示的，所以直接在Matplotlib中显示OpenCV图像会出现问题，因此需要转换一下 1234567891011121314151617import cv2import matplotlib.pyplot as pltimg = cv2.imread('lena.jpg')img2 = img[:, :, ::-1]# 或使用# img2 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# 显示不正确的图plt.subplot(121),plt.imshow(img) # 显示正确的图plt.subplot(122)plt.xticks([]), plt.yticks([]) # 隐藏x和y轴plt.imshow(img2)plt.show() img[:,:,0]表示图片的蓝色通道，img[:,:,::-1]就表示BGR翻转，变成RGB，说明一下： 熟悉Python的童鞋应该知道，对一个字符串s翻转可以这样写：s[::-1]，’abc’变成’cba’，-1表示逆序。图片是二维的，所以完整地复制一副图像就是： 1img2 = img[:,:] # 写全就是：img2 = img[0:height,0:width] 而图片是有三个通道，相当于一个长度为3的字符串，所以通道翻转与图片复制组合起来便是img[:,:,::-1]。 结果如下： 加载和保存图片不使用OpenCV，Matplotlib也可以加载和保存图片： 12345678import matplotlib.image as pltimg = plt.imread('lena.jpg')plt.imshow(img)# 保存图片，需放在show()函数之前plt.savefig('lena2.jpg')plt.show()","link":"/2020/03/30/opencv2/"},{"title":"pytorch使用说明","text":"torch.normal(means, std, out=None) 返回满足正态分布的张量 means和std分别给出均值和标准差 torch.rand(*sizes, out=None) → Tensor 均匀分布：返回一个张量，包含了从区间[0, 1)的均匀分布中抽取的一组随机数。张量的形状由参数sizes定义 torch.randn(*sizes, out=None) → Tensor 标准正态分布：返回一个张量，包含了从标准正态分布（均值为0，方差为1，即高斯白噪声）中抽取的一组随机数。张量的形状由参数sizes定义 torch.squeeze(a,N) 对数据的维度进行压缩，去掉指定个维数为1的维度，注意：如果不加第二个参数’N’，就是去掉所有维数为1的维度 另一种形式，a.squeeze(N) 也是去掉a中指定的维数为一的维度，去掉的维度数为N torch.unsqueeze() 对数据维度进行扩充。给指定位置加上维数为1的维度,同样有另外一种形式a.squeeze(N) 注意！当对神经网络的输入层的特征数据提前处理时，可能需要使用该函数，因为输入数据必须是一个至少二维的张量 可以关注一下python自带的numpy数字计算库和torch的数据处理，其实相似度很高 torch.cat((A,B),0) 将两个张量（tensor）拼接在一起，第一个参数是两个张量组成的元组，第二个参数是行拼接(1)或者列拼接(0) 实例 np.hstack((A,B,C..)) &amp; np.vstack((A,B,C..)) 将参数元组的元素数组按水平/垂直方向进行叠加，可以和上面的torch.cat比较一下 torch.stack([tensor1, tensor2, tensor3…], dim=0) 将几个矩阵按照dim指定维度堆叠在一起 实例 np.random.shuffle(x) 现场修改序列，改变自身内容。（类似洗牌，打乱顺序） 对多维数组进行打乱排列时，默认是对第一个维度也就是列维度进行随机打乱 np.transpose(x) 常用于二维数组的转置 torch.div(input, value, out=None) 将input逐元素除以标量值value，并返回结果到输出张量out。 plt.scatter(x,y,c='r',s=20,marker=&quot;.&quot;,lw=2) matplotlib库中画散点图的函数，注意c这个参数的特别用法 详解","link":"/2020/04/07/pytorch%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/"},{"title":"python学习-3","text":"enumerate()函数:用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在for循环当中 sorted()函数:对所有可迭代的对象进行排序操作（链接内含sort()函数与sorted()函数区别） lower()函数:将字符串中所有字母进行小写操作，（不能对其他类型数据进行操作，会报错） items()函数:以列表返回可遍历的(键，值)元组数组，(xxx.items()) lambda : 匿名函数 装饰器（Decorator）是通过python的@语法实现的 OOP（Object Oriented Programming）：面向对象的编程，是一种程序设计思想 数据封装、继承和多态只是面向对象程序设计中最基础的3个概念 Python允许使用多重继承，因此，MixIn 就是一种常见的设计 如果想将一个类用作for … in循环，类似list或tuple，那么必须有一个__iter__()方法来返回一个迭代对象，但是要注意他就是一个类，当然经过实验发现你通过类建立实例也可以等价的做for循环，注意__iter__要配合__next__使用 我们自己定义的类表现得和Python自带的list、tuple、dict没什么区别，这完全归功于动态语言的“鸭子类型”，不需要强制继承某个接口。 __call__()还可以定义参数。对实例进行直接调用就好比对一个函数进行调用一样，所以你完全可以把对象看成函数，把函数看成对象，因为这两者之间本来就没啥根本的区别 通过callable()函数，我们就可以判断一个对象是否是“可调用”对象。 Python的class允许定义许多定制方法，可以让我们非常方便地生成特定的类。 本节介绍的是最常用的几个定制方法，还有很多可定制的方法，请参考Python的官方文档。 借用一个例子深刻理解__str__、__repr__、__getattr__、__call__ 1234567891011121314151617输出个性化URL:class Chain(object): def __init__(self, path=''): self.__path = path def __getattr__(self, path): return Chain('%s/%s' % (self.__path, path)) def __call__(self, path): return Chain('%s/%s' % (self.__path, path)) def __str__(self): return self.__path __repr__ = __str__ print(Chain().users('michael').repos) # /users/michael/repos python的枚举类型 正常情况下，我们都用class Xxx...来定义类，但是，type()函数也允许我们动态创建出类来","link":"/2020/03/18/python-3/"},{"title":"python学习-2","text":"函数的各种参数类型，廖雪峰的博客 今天主要学习了函数，函数参量的种类和一些注意事项 From xx import xxx 可以从xx.py文件中引用函数xxx(…) isinstance(object, classinfo) object-实例对象、classinfo-可以是直接或间接类名、基本类型或者由它们组成的元组；相比于type()，建议使用isinstance() x**n 变量的高次幂的表示形式 def f2(a, b, c=0, *, d, **kw): 函数的参数组合，注意顺序；此外，对于任意函数，都可以通过类似func(args, *kw)的形式调用它，无论它的参数是如何定义 递归函数，思路很重要，自己的理解能力还是太弱了，加强锻炼吧！ 递归函数的经典问题：汉诺塔；例程函数如下 123456789def hanoi(n,a,b,c): # 当n为1时 (递归基础) if n == 1: print(a, '--&gt;', c) # 将A柱最底层的圆盘移动到C柱 # 当n大于1时 else: hanoi(n-1, a, c, b) # 借助C柱，将n-1个圆盘从A柱移动到B柱 print(a, '--&gt;', c) # 将A柱最底层的圆盘移动到C柱 hanoi(n-1, b, a, c) # 借助A柱，将n-1个圆盘从B柱移动到C柱 range()函数用法 range(start, stop[, step]) 基本用于for循环,生成列表或元组的一般表达形式list(range(x,x,x))或tuple(range(x,x,x))","link":"/2020/03/13/python%E5%AD%A6%E4%B9%A0-2/"},{"title":"pytorch深入探索续篇","text":"前言&emsp;&emsp;由于前一段时间在搞定位算法，暂时没有时间来学习、记录pytorch的学习过程。在这里就先提及需要详细解释的一些部分 优化器optimizer的种类、各自的优势，以及我应该如何选取 正则化、过拟合、batch normalization的解释和应用 学习更多的深度学习算法来更好的应用在实际例子中 更多pytorch例程的研究记录","link":"/2020/04/09/pytorch%E6%B7%B1%E5%85%A5%E6%8E%A2%E7%B4%A2%E7%BB%AD%E7%AF%87/"},{"title":"pytorch、jupyter初识","text":"前言&emsp;&emsp;如何逐渐掌握深度学习呢，是一个难题，网上参差不齐没有一个非常好非常全面的教程，只能一点一点自己累计，寻找例程来模仿主要理解他是怎么一步步完成任务的、思考一个普遍性的解决问题的过程，如何搭建深度学习的神经网络框架，现在暂时选择pytorch。&emsp;&emsp;然后，目前推荐搭建框架过程使用jupyter notebook，所以也要学习如何使用它，那么我想在每一阶段都解决和思考一些问题记录下来，来一步步稳扎稳打的学习深度学习 个人思路&emsp;&emsp;其实，深度学习说难不难，说简单肯定不简单，你真的可以甚至1天快速入门、上手，只要你懂一些python语法，稍微看看比较通俗的神经网络的介绍文章，学一种搭建框架的方式就能让你用上你设计的网络结构，大批量的规则化的图像信息甚至也可以从网上获取，然后非常方便的使用、训练、测试。&emsp;&emsp;但是你想真正用神经网络来解决问题你自己的实际问题，那么这些将远远不够，至少我是这样认为的，所以我寻找了很多比较有顺序的深度学习教程（pytorch的），都需要一篇篇的仔细阅读，深化概念，让之后的进阶操作能更快理解。 学习阶段————初次接触思考： 如何将一大批尺寸不一、随机的图像统一标准化的作为深度学习的输入、以及同时集合这些输入的label–对应的解，即真实值（来和输出层的计算值比较损失）？ 神经网络的框架如何根据自己的要求、向适合解决问题的网络结构改进？ 学习如何使用GPU来训练模型？ jupyter notebook 中如何正常运行pytorch网络？ 一、在Morvan莫烦的pytorch教程中，再次理解神经网络1.1 神经网络是什么 之前虽然理解了一种说法，好像没提出来，这里讲一讲，但不讲太多，没啥意义 首先，人们希望模仿人类的大脑，即生物神经网络，人类如何形成条件反射？如何学习？如何记忆？大脑内神经元如何工作？神经元之间有什么样的联系？ 人工神经网络相比，相似，又不相似，所有神经元的连接在同一时间都是固定的，不会凭空产生新联结，它是一种逐次接近正确答案的训练手段，其中“误差反向传递”起到关键作用 1.2 神经网络的简单结构概念 Gradient Descent：梯度下降&emsp;&emsp;&emsp;&emsp;Cost Function：损失函数 输入层、隐含层（第1层。。。第n层）、输出层就对应着特征–&gt;代表特征1–&gt;…–&gt;代表特征n–&gt;输出，这些代表特征是越来越抽象的，人越来越无法理解，但计算机就对他比较敏感了 迁移学习：即在当你现有的一般神经网络足够处理比如图片内容种类识别、字符识别的任务后。如果我们在输出层前再加入几层神经网络来训练，就能来完成进阶的任务，比如不仅识别数字，还能知道数字的书写字体。。等等 2.1 选择Pytorch 好用、使用广泛、新颖、直观、动态建立框架 torchvision里已经有一些它搭建好的网络，比如resnet（可以可以，我知道例程是这样用的了），然后你用这个已有的网络来训练自己的任务 将jupyter notebook工作目录更改为自定义，即在属性栏启动目录将userprofile/..双引号内的内容改为自己希望前往的目录地址，重启即可 pytorch常用函数的详解&emsp;&emsp;&emsp;&emsp;numpy矩阵类型与pytorch中tensor（张量）矩阵转换 2.2 什么是Variable &emsp;&emsp;它在神经网络的训练中是一个关键，为什么这么说？思考一下，神经网络的反向传播算法常用梯度下降法，那其中如何求梯度呢？&emsp;&emsp;大家都知道就是求偏导数，导数就是梯度，torch里求导数的小帮手就是variable，将tensor张量放进variable中，开启自动求梯度的配置 1variable = Variable(tensor,requires_grad=True) #requires_grad = True 那么，你以后含variable的运算就都和variable本身建立了某种联系，可以快速得到以后的结果相对variable的梯度 12v_out = torch.mean(variable*variable) #例如求个平均数的运算v_out.backward() #求梯度 variable变量不同于tensor，但是variable.data可以得到tensor，如果想进行tensor和numpy_array的转换，那么就要xx.data先得到tensor variable的运算和tensor相似，但不完全相同 2.3 激励函数(AF–activation function)？ &emsp;&emsp;激励函数广泛应用在各种层，各种用，可是为什么要激励函数呢？因为复杂问题中，输入输出往往都不是线性关系，不是线性，你就不能简单在他们中间乘个系数。&emsp;&emsp;进阶的操作就是，在输入做完简单线性处理（乘k）的结果代入一个非线性函数 &emsp;&emsp;常用的非线性激活函数由relu、sigmoid、tanh方程，甚至可以自己创造激励函数来求解，不过要注意，这些激励函数必须都是可以微分的，因为需要求梯度求导 &emsp;&emsp;当隐藏层不多时，可以尝试任意一个激励函数；不过当隐藏层中含有非常多个layer，就有慎重选择，可能出现梯度爆炸、梯度消失（我也不太懂，再学学）。其次，具体案例中，少量隐藏层可以有多种选择，卷积神经网络首选relu，循环神经网络首选relu、tanh 如何应用activation function 理解代码然后实验即可1234567891011121314151617import torchimport torch.nn.functional as Ffrom torch.autograd import Variableimport matplotlib.pyplot as plt# fake datax = torch.linspace(-5, 5, 200) # x data (tensor), shape=(100, 1)x = Variable(x)x_np = x.data.numpy() # numpy array for plotting# following are popular activation functionsy_relu = torch.relu(x).data.numpy()y_sigmoid = torch.sigmoid(x).data.numpy()y_tanh = torch.tanh(x).data.numpy()y_softplus = F.softplus(x).data.numpy() # there's no softplus in torch 3.1 构建网络：回归 利用Pytorch建立一个简单的神经网络 12345678class Net(torch.nn.Module): def __init__(self, xxx, xxx, xxx, ...): super(Net, self).__init__() # xxxxxxxx def forward(self, x): # xxxxxxxx return x 定义一个实际的神经网络 1net = Net(xxx=1, xxx=10, xxx=1, ...) 定义优化器optimizer和损失函数loss function 12optimizer = torch.optim.SGD(net.parameters(), lr=0.2) #net.parameters()：包含了神经网络的所有参数 lr：学习率（步长）loss_func = torch.nn.MSELoss() # this is for regression mean squared loss 那么在训练的时候 首先定义网络的输出位置prediction，真正计算损失loss 每次利用优化器清除掉网络中之前计算的梯度 然后开始运行反向传播算法 最后向正确方向迈出一步，用计算出的梯度再计算出新一轮的网络参数 12345prediction = net(x) # input x and predict based on xloss = loss_func(prediction, y) # must be (1. nn output, 2. target)optimizer.zero_grad() # clear gradients for next trainloss.backward() # backpropagation, compute gradientsoptimizer.step() # apply gradients 当然本节是对于简单的回归问题，所以定义输出量的变量简单，损失计算也是简单的均方差算法 3.2 构建网络：分类 与简单的回归神经网络不同的是，分类神经网络的输入量和输出量都不只有一个 输入量可以是一组坐标，甚至可以是一张图片的一个三维张量，而输出则可以是一个输入张量对所有类别的概率判断 分类的损失函数选择和回归是不一样的，教程中选择了loss_func = torch.nn.CrossEntropyLoss() 当然，对于我要解决的字符识别问题而言，输入就要复杂得多，远不是一对坐标那么简单，输出的类别也会有很多，所以更透彻的学习将在第二阶段进行","link":"/2020/04/03/pytorch%E5%88%9D%E8%AF%86/"},{"title":"基于matlab的蓝牙串口通信","text":"一、前言&emsp;&emsp;用蓝牙做主控串口和电脑通信的想法呢，是因为实践标定的时候需要持续的很稳的推动小车，而且要把jlink将电脑和主控连起来debug看数据又得给主控供5V电（当时忘记充电宝可以供5v电了，一时英明被摧毁），所以无线就没多想了。想过做很长的线、把电脑放在电竞椅上跟着小推车推着走，都不太靠谱，所以在标定的时候还是把电脑架在了小推车上 &emsp;&emsp;标定进度完成了大半，现在得点空，还是想把这个蓝牙无线通信做起来，因为我在想这个方法是不是可以把WLan的网卡看参数替换掉，个人感觉那种Labview结合Matlab🐂是🐂，但是有点太麻烦了，路由器架在车上得自带一个充电宝，主控上的网线插槽老是松，还要先生成调试组文件带入主控更新…我电脑现在也是再也连不上了，所以这个我觉得某种程度上有一些必要 二、尝试pybluez模块&emsp;&emsp;之前竞培营的时候买过蓝牙，所以家里留了一个（也是为什么想蓝牙通信）。主控串口连接蓝牙再和电脑通信，问题不在串口，而在电脑是如何接收到的数据？什么形式的数据？怎么处理这类数据成为我想要看到的数据甚至生成动态曲线？由于觉得python无所不能，第一时间想到的就是它，结果在下载pybluez模块时出现问题，电脑提示我没有Microsoft Visual C++ 2014的组件工具，这个我百度了也是很多py模块下载时出现的问题 &emsp;&emsp;有一个解决方法就是在一个国外大学提供实时更新的模块下载网站上先把需要的模块的类似安装包（whl类型文件）下载到本地，之后pip install 安装包绝对地址最后pip install 模块名称，不过这个方法我在尝试的时候仍然出现错误，个人认为是这个pybluez模块版本较老，有一些bug也比较正常 &emsp;&emsp;这条路走不通，以后必须用到的时候再试试把。之后在想wifi也能无线传输数据，而且速率更高，pywifi模块的安装没有问题。不过我手上只有蓝牙模块啊，当务之急是选择什么方法在电脑上得到蓝牙传输的数据 三、Matlab成为选择&emsp;&emsp;此时，在寻找与蓝牙通信相关的过程中，看到了Matlab的字眼，对啊，之前怎么没想到它呢。说干就干，百度关键词Matlab蓝牙通信，果然就有，不过不多，好在matlab的使用者多，而且内置的help搜索向导非常方便，即使不太了解有的也有官方示例。 123456789101112131415161718192021222324%Examples%Find available Bluetooth devices.instrhwinfo('Bluetooth');instrhwinfo('Bluetooth', RemoteName); %Construct a Bluetooth object called b using channel 3 of a Lego Mindstorm robot with RemoteName of NXT.b = Bluetooth('NXT', 3);%Connect to the remote device.fopen(b)%Send a message to the remote device using the fwrite function.fwrite(b, uint8([2,0,1,155]));%Read data from the remote device using the fread function.name = fread(b,35);%Disconnect the Bluetooth device.fclose(b);%Clean up by deleting and clearing the object.fclose(b);clear('b'); 结合官方提供的例子，快速入手蓝牙通信，关键在于定义蓝牙的特定变量b = Bluetooth('NXT', 3);，以及fopen(b)来开启蓝牙通信。重点就在于数据的规范、格式。手册上介绍的函数如下： You can read and write both text data (ASCII based) and binary data. For text data, use the fscanf and fprintf functions. For binary data, use the fread and fwrite functions.你可以借助ASCII码或者二进制数据进行读写操作。对于txt数据使用fscanf、fprintf函数，对于二进制数据使用fread、fwrite函数 以下是目前我脚本中的蓝牙初始化操作 12345678910111213141516171819202122232425%% 配置蓝牙操作instrhwinfo('Bluetooth');instrhwinfo('Bluetooth','col_test');col_test = Bluetooth('col_test',1);% col_test.TimerPeriod = 0.1;% col_test.TimerFcn = {@plotcallback,p};col_test.Timeout = 10;%% 开启蓝牙数据监控，同时开启定时器% fopen(col_test);try fopen(col_test);catch err disp('col_test蓝牙通道打开失败'); flag = 0;endif err_flag disp('col_test蓝牙通道成功打开'); fwrite(col_test,Command_data); start(t);end 通信接口的回调函数 在搜索帮助中有对串口接收数据的中断回调函数，终端的类型我所了解的有 设定接受字节数触发 设定截至字符触发 定时间间隔触发 不过目前没尝试出来，而且我觉得也没有那个必要所以暂时搁置了研究 四、如何有效的接受主控数据&emsp;&emsp;如果需要监控主控数据，那么每一次就要同时接收数十个浮点数类型的数据。 接受的方式不可能是直接不停循环接收数据，那么对于高速率传输数据的串口，错误必然存在而且经验证发现错误率不低，所以需要加密再解码确保数据准确。 其次，由于测试发现fread函数接受蓝牙数据是以字节（8bit）为单位，多个字节数据如何正确转换为float类型我之前没接触过。 最后蓝牙的速率不出众，那么我也需要测试一下他的速度究竟能有多快？ 4.1 报文形式发送与接收 主控上的串口dma接收发送不必多说，使用串口六，定义两个结构体Bluez_TX、Bluez_RX包含同样的两帧头两帧尾，那么在Matlab脚本中就需要相似的对于报文的解包，这一部分不难，那么我认为matlab接收数据的优点在于in/outputbuf数据一边进另一边出，在读取过程中的操作十分方便。 还有一点就是我的脚本中做到，matlab向主控发送一帧数据，命令主控开始发送数据给蓝牙从而开始数据监控。也有停止命令，非常方便。操作就是在主控中的dma发送函数判断一个标志位（发送函数写在一个任务中） 4.2 多个单字节与浮点数的转换 已知，主控中的dma发送是将发送结构体变量bluez_tx的头指针作为地址，那么这种方式下浮点数就是按一般的占据4字节进行发送。在Matlab中可以接收到4个字节与该浮点数对应。 123raw_data1 = [byte1 byte2 byte3 byte4];temp = uint8(raw_data1);stRobot_x(count) = typecast(temp,'single'); 通过上述方法进行通信之后的变量类型转换成功 4.3 速率监测 一开始我都是用串口调试助手来研究的，没有用上主控，中间尝试过一次发现主控确实在发送数据，但是Matlab端就是感觉没接收到，很是奇怪。那么我用调试助手时发现，一个包里有单个数据或是多个数据并不影响接收帧率，增大串口波特率略微提高解包帧率，但是在发送帧率到达1000fps时，数据的接收帧率只能到650fps左右。 我发现，并不是Matlab没接收到主控的数据，而是没有成功解包，这个问题也一直困扰我，发送结构体变量中的总字节数理论上和实际发的不一样，在帧头和数据之间有两位0x00并不知道从何而来，这样总字节数多了2，dma发送字节数加了2，且在解包过程中多两个字节消耗掉就行了 结果在测试主控与Matlab通信时又发现问题，由于设置的波特率为921600，虽然配置没错但是串口发送的数完全不对，向下减小一些至460800就正常了，看来这种情况波特率有了限制。并且波特率460800时帧数据肉眼可见的概率会发生传输数据错误，说明串口确实不太稳定 最后我一共让主控一帧传输20个数据，能够正常接收，100fps的发送帧率对应100fps的解包帧率。再次说明数据量不影响接受帧率，并且最大帧率在600fps（发送帧率1000fps），不过这种情况在接受10s时间左右程序不再接受蓝牙数据，未找到原因。 另外，如果要在接收过程中画出实时曲线，势必影响接收帧率，我尝试在发送帧率100fps的情况下，每解完一次包绘制两个变量的曲线，结果接受帧率掉到70fps。因此，不画实时图像或者仅绘制1-2张是允许的，不然影响过大。 4.3.1 帧率计算timer Matlab中也有定时器，我通过定时器1s定时来计算每秒钟的接收帧率，特别提醒往往回调函数的函数名的定义变量规范很重要。具体可以查阅帮助，下面是代码部分： 1234567t = timer('StartDelay',1,'TimerFcn',@t_TimerFcn,'Period',1,'ExecutionMode','fixedRate');%StartDelay：开始计时后的延时；TimerFcn：定时中断函数；Period：定时时间周期；ExecutionMode：优先级start(t)stop(t)Delete(t) 4.3.2 打印变量disp()12string = sprintf('Have received stRobot_x：%d',stRobot_x(count));disp(string) 类似于format格式化输出，打印在命令行区","link":"/2020/04/14/%E5%9F%BA%E4%BA%8Ematlab%E7%9A%84%E8%93%9D%E7%89%99%E4%B8%B2%E5%8F%A3%E9%80%9A%E4%BF%A1/"},{"title":"python 之 opencv-简单读取操作","text":"前言这几天看了很多关于图像处理的知识，知道神经网络是怎么回事，深度学习是什么东西，机器学习和他们有什么联系，又看了python语言下如何进行图像处理。我认为在接受更高层次的处理手段和算法设计前，应该打好基础，python的图像处理，学好PIL库和opencv库，我在github上找了个opencv的教程，接下来就是对opencv重点的一些备注 安装 pip install opencv-python 安装测试 python --versionimport cv2&emsp; or&emsp; print(cv2.__version__) # '3.4.1' 使用清华镜像在python中pip安装 临时使用： 可以在使用pip的时候加参数-i https://pypi.tuna.tsinghua.edu.cn/simple 例如：pip install -i https://pypi.tuna.tsinghua.edu.cn/simple gevent，这样就会从清华这边的镜像去安装gevent库。 opencv入门 OpenCV中彩色图是以B-G-R通道顺序存储的，灰度图只有一个通道 图像坐标的起始点是在左上角，所以行对应的是y，列对应的是x 12345678910111213import cv2# 开始计时start = cv2.getTickCount()# 读入一张图片img = cv2.imread('lena.jpg')# 停止计时end = cv2.getTickCount()# 单位：sprint((end - start) / cv2.getTickFrequency()) cv2.imread1234import cv2# 加载灰度图img = cv2.imread('lena.jpg', 0) 参数1：图片的文件名 如果图片放在当前文件夹下，直接写文件名就行了，如’lena.jpg’ 否则需要给出绝对路径，如’D:\\OpenCVSamples\\lena.jpg’ 参数2：读入方式，省略即采用默认值 cv2.IMREAD_COLOR：彩色图，默认值(1) cv2.IMREAD_GRAYSCALE：灰度图(0) cv2.IMREAD_UNCHANGED：包含透明通道的彩色图(-1) 经验之谈：路径中不能有中文噢，并且没有加载成功的话是不会报错的，print(img)的结果为None，后面处理才会报错，算是个小坑。 cv2.imshow、cv2.waitKey 使用cv2.imshow()显示图片，窗口会自适应图片的大小： 12cv2.imshow('lena', img)cv2.waitKey(0) 参数1是窗口的名字，参数2是要显示的图片。不同窗口之间用窗口名区分，所以窗口名相同就表示是同一个窗口 cv2.waitKey()是让程序暂停的意思，参数是等待时间（毫秒ms）。时间一到，会继续执行接下来的程序，传入0的话表示一直等待。等待期间也可以获取用户的按键输入：k = cv2.waitKey(0) cv2.namedWindow 我们也可以先用cv2.namedWindow()创建一个窗口，之后再显示图片： 1234# 先定义窗口，后显示图片cv2.namedWindow('lena2', cv2.WINDOW_NORMAL)cv2.imshow('lena2', img)cv2.waitKey(0) 参数1依旧是窗口的名字，参数2默认是cv2.WINDOW_AUTOSIZE，表示窗口大小自适应图片，也可以设置为cv2.WINDOW_NORMAL，表示窗口大小可调整。图片比较大的时候，可以考虑用后者。 cv2.imwrite 使用cv2.imwrite()保存图片，参数1是包含后缀名的文件名： 1cv2.imwrite('lena_gray.jpg', img) 练习 打开lena.jpg并显示，s如果按下’s’，就保存图片为’lena_save.bmp’，否则就结束程序 123456789import cv2img = cv2.imread('lena.jpg')cv2.imshow('lena', img)k = cv2.waitKey(0)# ord()用来获取某个字符的编码if k == ord('s'): cv2.imwrite('lena_save.bmp', img)","link":"/2020/03/29/opencv1/"},{"title":"标定记录 2nd","text":"2020.4.27 陀螺仪问题解决 先做线（杜邦线），用2s锂电池单独给陀螺仪供电后问题解决 每次转一圈的数据比较稳定，每转1080°陀螺仪若干数据之间的差不超过20 再次标定 由于陀螺仪数据稳定，需要再次标定，标定后角度经验证比较准确： 每转三圈读取的角度值在1080°±3°之间浮动 角度标准不同后，还需要整体重新标定，结果图示如下： 验证正y轴： y方向行进约 1300 mm，x方向偏移量约为 0.7 mm 验证假x轴： x方向行进约 1400 mm，y方向偏移量约为 10 mm 验证正y轴转为假x轴后沿正y轴走： 这里最终x方向偏移量约 4 mm 根据上图走假x轴的x、y位移数据 acrtan（10.04 / 1373）= 0.4189653° ​ 又推车转角为 89.5257507° 89.5257507° + 0.4189653° = 89.944716° ≈ 90° ​ 在这里再次验证了之前的结论 重复性误差 在这里我推车测试了8条不同的路径（左下角到右上角），并且把每条路径最终的x、y坐标都列了出来，横线代表8个值的平均值，由于条件有限无法知道精确的实际值，所以把平均值当作实际值。 结果发现，在推车直线距离大概 1.5 m的情况下，小车在x、y方向都可能存在5mm内的偏移 旋转误差 这里每次旋转 3 - 4 圈后回到起始点，来观察小车的定位误差情况 顺逆时针都测试了，发现顺时针坐标整体偏移了 5 mm，然而逆时针坐标整体偏移了约 20 mm 为何会有偏移？ 由上图可以发现转三圈的角度值都是1080°左右，说明陀螺仪提供的角度数据没有问题 旋转算法matlab程序实现是否有问题？ 我把程序中 alpha 和 L 的值都设为 0，再次转3圈看结果： 结论是与旋转算法无关，测试结果照样有十几到二十多毫米的偏移 （这里发现路径图中由于是随动中心，会绕中心转圈，是对的，之前是车的中心，所以路径图比较奇怪） 直行算法matlab程序实现是否有问题？**（由于在家标定时旋转角度方向变了，我改了一部分matlab代码，所以怀疑是不是这个有问题） 我把之前 Adams 的仿真理想数据带入现在的matlab程序，并且在程序里再次分别验证走正y轴，假x轴：正y轴的x方向偏移 0 mm；正x轴的y方向偏移 1.2 mm 比较理想，证明程序无误 综上，那就意味之标定的系数（8个）还是不准，那就再小心的标定一次 再再次标定 这次标定不一样，电脑不用放在车上了，标定也用蓝牙传数据（这周第一次整体标定电脑还是放在车上的）；并且，走一个方向多次重复取平均值。 这一次标定效果如下： 验证正y轴： y方向行进约 1400 mm，x方向偏移量约为 0.25 mm 验证假x轴：（这次两边测得正好是90°，没办法每次都测的不一样，这个车有一边有问题） x方向行进约 1400 mm，y方向偏移量约为 1.293 mm 验证正y轴转为假x轴后沿正y轴走： 这里最终x方向偏移量约 4 mm，和这周第一次标定的效果 再次看看旋转的误差大小 旋转三圈回到起始点，依然会有 10 - 20 mm的坐标偏移 结论 &emsp;&emsp;考虑到标定时的车身的振动，旋转时车身的振动，自身操作使随动轮前后小幅度摇摆造成回程差等等的条件受到限制。 &emsp;&emsp;这一套系统的精度目前看来就是上述的研究结果。","link":"/2020/04/27/%E6%A0%87%E5%AE%9A%E8%AE%B0%E5%BD%95-2nd/"},{"title":"图像识别与处理","text":"前言：图像识别与处理的大实验，需要完成的任务是基于嵌入式的字符识别 我目前想到的方案： 硬件方面：利用手里有的stm32f4开发板，再买一个ov2460的摄像头采集图像，最后借助手上的蓝牙模块传输信息到电脑上。 算法部分：做基于python的程序编写，那么准备利用好github上的各种方法，将这一块的图像处理学透,比如得到一个原始图像的初步处理，后面的支持向量机（SVM）、深度学习等 CNN一个通俗易懂的卷积神经网络的内容介绍 这是一个非常容易理解的CNN卷积神经网络的入门介绍，一个图像的信息以RGB的形式输入，那么就是一个3维的且“厚度”为3阶（一个RGB像素点含三个数）的矩阵，经过 卷积层（矩阵样式的过滤器，很好理解） 激活函数（比如relu，对每一层后得到的矩阵处理） 池化层（分成中块对每个中块进行处理，来简化阶数） 全连接层（涉及传统的神经网络，这一层是每一个单元都和前一层的每一个单元相连接，所以称之为“全连接”） 到了全连接层之后的操作，就跟普通的神经网络无异了,卷积神经网络比较于传统神经网络的优势在于： 1.参数共享机制：对于不同的区域，我们都共享同一个filter，因此就共享这同一组参数；有效地避免过拟合；由于filter的参数共享，即使图片进行了一定的平移操作，我们照样可以识别出特征，这叫做 “平移不变性”。因此，模型就更加稳健了 2.连接的稀疏性：传统神经网络中，由于都是全连接，所以输出的任何一个单元，都要受输入的所有的单元的影响。这样无形中会对图像的识别效果大打折扣。比较，每一个区域都有自己的专属特征，我们不希望它受到其他区域的影响 Logistic Regression逻辑分析的理解 逻辑分析的含义： 有一个列向量，其中包含着一张照片的所有有效信息，他可能有10个数据，也可能有10000个数据，通常是像素值；每一个数据都对应着一个权重，那么自然这些权重也会组成一个列向量。所有数据加权求和之后加上一个系数b就会有一个输出，可以理解成一张照片就会有一个总输出 x0w0+x1w1+…+x12287w12287+b=WTx+b 对于总输出的处理，他会作为一个激活函数（对于神经元的）（对于逻辑分析就指的是sigmoid函数）的变量输入，函数的输出就是这张照片的一个预测结果y’，sigmoid函数**输出在0-1范围内，对于图像处理最终结果的判断就在0-1中且分为了几个区间，y’在这个区间说明图片是“xx”，在那个区间又说明什么。。。 y’ = σ(WTx+b) 而其实这都是根据一个前置处理（加权求和、带入激活函数）得出的预测值，他就不会一定准，所以有一个损失函数来衡量y’与真实值y（比如sigmoid函数输出对应的0或1）的差距 L(y’,y) = -[y·log(y’)+(1-y)·log(1-y’)] 那么对于一个我们用来训练的图片集来说，每一张照片都有一个预测值y’。这些y’的损失函数的输出值的平均值尽量小，那就是综合了所有图片数据得到的模型。寻找W和b的较好的值，就靠这个平均值，也叫平均损失（其函数名称叫代价函数）求得，这个函数的导数为零的那一点应该是极小值，那损失就是极小值。 J(W,b) = 1/m·Σmi=1L(y’(i),y(i)) 通常用梯度下降法，那么操作起来就是，求得函数在目前的W、b处对W的导数，如果梯度（导数）为正，说明曲线之后向上走，所以W的值应该减小才能往低处走，梯度为负那就往前走正好是往低了走，重复一次成为一个迭代，当然还有对b的偏导，一次次迭代，一次次更新W和b的参数。梯度下降法中学习率（α）相当于每次迭代步长 w := w - α(dJ/dw) 最后那这对（W、b）就可以用来检验对于图像识别的准确率了 一个对逻辑回归分析更清晰的介绍 简单来说， 逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。 上文所提到的代价函数称之为交叉熵，是对「出乎意料」（译者注：原文使用suprise）的度量。那么，交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。 sigmoid函数为决策边境，他是线性的。 NNS传统神经网络的基础入门 作为对前面碎片化学习的延申。我认为只能说一次逻辑回归可能只能应用于线性问题，而有多个计算层的神经网络是可以解决非线性问题的。这是由于每一个计算层，或者说每一个单元的处理都是线性的，但是前一个计算层将数据空间扭曲了（两层间的权重矩阵是转换的关键），使得下一个计算层就算是线性的，也可以很好的划分出决策边境。因此，多层的神经网络的本质就是复杂函数拟合 神经网络通常有 输入层（包含已知的若干特征） 隐藏层（对输入特征一层层的抽象，获得更深一层的含义） 输出层（希望获得的目标，可以是单一值，也可以是向量） 偏置单元：它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。 对于只有输入层和输出层的神经网络称为感知器，它只能做线性的分类任务；而多层神经网络（深度学习）就能更深入的表示特征，以及拥有更强的函数模拟能力 传统神经网络中一个单元（节点 or 神经元）的处理基本上可以陈述为：上一层所有节点的输出向量矩阵乘上两层之间的权重矩阵，得到的结果矩阵，再作为激活函数（转移函数）的变量输入。函数输出值作为这个节点的输出向下一层传递、或者作为目标直接输出 在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。 ReLU函数：不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。 训练：训练的主题仍然是优化和泛化。 优化：梯度下降算法以及反向传播算法（BP）在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化。 反向传播算法:即由后层向前层一层层的递推，递推什么呢，就是代价函数对于这些个权重的偏导数，由于高等数学中的偏导数中的链式法则 泛化：在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现过拟合现象。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。 深度学习就是多层神经网络，而机器学习则不同，他大概是人工智能算法的一个总称，他的算法包含了回归算法、神经网络、SVM等等，是一个宽泛的课题 深度学习之卷积神经网络经典模型","link":"/2020/03/25/%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB%E4%B8%8E%E5%A4%84%E7%90%86/"},{"title":"标定记录","text":"2020.4.13 &emsp;&emsp;周末测试的定位算法，算是一个阶段性的理解。发现了一些问题并且改正；其中包括： 在标定过程中，以近y轴为基准的话，那么旋转来测量x轴行进数据时，需要特别注意旋转方向 以前测量时，全都是顺时针旋转约90°，所有公式都是以顺时针旋转来推导的，原本以为只要规定了正方向，结果应该都一样 但是由于昨天测量时是逆时针，在结算代入数据时发现求出的数据不对，重新推算了一遍发现顺逆时针旋转的算法结算有略微差别 本次差别在于：x轴行进时的A、B轮相对行进方向角度的余弦值，顺逆时针是相反地 下图为两种旋转方向的图示： 这里备份了两种相似算法的matlab程序逆时针算法顺时针算法 旋转方法也进行了测试，之前一直没有一个比较具体的有效的验证 首先，采用的验证方法是已知小推车的模型参数，从而得到理论上的几何中心和随动中心的距离L、以及对应夹角 条件是在测完直行标定，并计算出8个变量带入程序；并且程序中的L、alpha改成0 其次，操作小推车旋转约180°（顺逆时针皆可）旋转前后都会有一条铝管作为基准边，得到计算出的x、y坐标的变化 带入matlab旋转算法结算程序，求得L与alpha（注意matlab中的alpha为顺时针为正方向计算，程序中需要取负号） 将理论和计算出的长度、角度变量比较，发现长度误差1.2mm，角度误差1.2°，考虑到加工精度误差、以及人为操作误差，结果可以接受 这里备份了旋转算法的matlab程序&emsp;&emsp;&emsp;&emsp;旋转算法 —理论数据— 横间距：37.5 mm 长间距：-29.84 mm 矢量绝对值：47.9236 mm 夹角：-2.24293 rad ( -128.5104 °) —计算数据— 矢量绝对值：46.698439058181290 mm 夹角：-2.222503658302204 rad ( -127.33 °)","link":"/2020/04/13/%E6%A0%87%E5%AE%9A%E8%AE%B0%E5%BD%95/"},{"title":"pytorch深入了解","text":"前言&emsp;&emsp;经过这几天的零碎学习，我真是觉得神经网络的搭建真的太复杂了，复杂到什么程度呢？就是一个概念不懂，找到解释后，发现解释里又包含着好几个你根本不知道的概念，不过还好我做了一些学习准备，也不算是完全没有头绪。但是目前暴露的缺陷有： python高级语法知识的欠缺 pytorch搭建神经网络的细节问题不懂 对于神经网络的每一个相关细节概念的理解和了解不足 &emsp;&emsp;那么针对这些，我认为可以在学习一些实例、教程时碎片化的学习来完善自己的知识框架，所以有了以下的“深入了解”学习阶段 学习阶段————深入了解思考： 神经网络的框架如何根据自己的要求、向适合解决问题的网络结构改进？ 学习如何使用GPU来训练模型？ jupyter notebook 中如何正常运行pytorch网络？ 二、结合实例理解，弥补缺失的框架体系知识1. 读取文件必要的Dataset与DataLoader？ &emsp;&emsp;获取数据一直应该都是神经网络的一个要点。有些时候可以通过某些官方渠道的统一数据集来进行训练。但对于实际的项目，往往无法这样做，那么对于这一次我的大作业也就是获取图像信息来说，每一张图像的数据不一样，尺寸也都不一样，如何把这些杂乱的数据统一的放入神经网络中学习呢？在这里Dataset与DataLoader就起到了必要的作用 Dataset与DataLoader分别对应数据的读取和操作，他们是官网提供给我们处理数据的一个范例。 Dataset Dataset位于torch.utils.data.Dataset中，往往我们需要创建自己的获取数据集的Mydataset类，他必须继承官方的Dataset类，并且必须要实现其两个成员函数：__len__()、__getitem__() 12345678910111213141516171819202122232425262728293031323334353637383940414243444546import torchimport cv2from torch.utils.data import Datasetclass datasets(Dataset): def __init__(self,data,transform = None,test = False): imgs = [] labels = [] self.test = test self.len = len(data) self.data = data self.transform = transform for i in self.data: imgs.append(i[0]) self.imgs = imgs labels.append(int(i[1]) ) #pytorch中交叉熵需要从0开始 self.labels = labels def __getitem__(self,index): if self.test: filename = self.imgs[index] filename = filename img_path = self.imgs[index] img = cv2.imread(img_path) img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) img = cv2.resize(img, (config.img_width, config.img_height)) img = transforms.ToTensor()(img) return img,filename else: img_path = self.imgs[index] label = self.labels[index] #label = int(label) img = cv2.imread(img_path) img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB) img = cv2.resize(img,(config.img_width,config.img_height)) # img = transforms.ToTensor()(img) if self.transform is not None: img = Image.fromarray(img) img = self.transform(img) else: img = transforms.ToTensor()(img) return img,label def __len__(self): return len(self.data)#self.len 在这个实例中，在初始化时需要的参数是，需要训练或者测试的数据，transform操作（这个具体在后面说明），至于后面的test参数也和transform有关 重点看类里内置的方法 __getitem__(self,index):该方法支持从 0 到len(self)的索引&emsp;&emsp;在我们自己的Dataset中必须需要一个这个方法获取数据集中的每一组数据。比如在此例中，由于在建立dataset实例时出入的data数据是所有的.jpg文件，那么当我们需要获取图片进行训练或测试时，需要利用opencv的方法读取图片数据、将图片转换成RGB格式，把每个图片的尺寸统一为设定的模板尺寸，最后将图片信息数据转换为tensor张量形式 __len__(self)&emsp;&emsp;这个方法比较简单，用起来也很方便。目的就是，返回数据集的长度。但这个方法必须有。 现存问题：在初始化时，类里面的一些赋值操作，是否存在必要性。当然，数据必须保存在数据集中，所以一个循环读取全部数据内容的操作应该要有。所以我猜测是自己如果需要用到就加，如果不会用到就不需要写在初始化方法中。 DataLoader torch.utils.data已经提供的类：Dataset，但是通过这种方式只能一个个的数据的把数据全部读出来，定义了数据读取的方式，不能实现批量的把数据读取出来，为此pytorch有提供了一个方法：DataLoader()&emsp;&emsp;简单点说，就是通过DataLoader将Dataset中的数据集以每次Batch_size个大小的组，读取出来，以供训练 Dataloader本质是一个可迭代对象，使用iter()访问，不能使用next()访问，由于它本身就是一个可迭代对象，可以使用for inputs, labels in dataloaders进行可迭代对象的访问 我们一般不需要再自己去实现DataLoader的方法了，只需要在构造函数中指定相应的参数即可，比如常见的batch_size，shuffle等等参数。所以使用DataLoader十分简洁方便。 12345678910 #自己定义的函数：获取数据并将数据分为训练集和测试集两类test_list , train_list = get_files(config.data_folder,config.ratio)#训练集 需要数据增强 transform 使用之前定义好的input_traindata = datasets(train_list,transform = transform) train_loader = DataLoader(input_traindata,batch_size = config.batch_size,shuffle = True,collate_fn = collate_fn ,pin_memory = False,num_workers = 4)#测试集 不要数据增强 transform = Noneinput_testdata = datasets(test_list,transform = None)test_loader = DataLoader(input_testdata,batch_size = config.batch_size,shuffle = False,collate_fn = collate_fn,num_workers = 4) 上述例子可以看到，根据自己定义的dataset创建了训练和测试两个实例，训练需要数据增强而测试不需要，所以两个实例除数据不同，tranform也不同 DataLoader的参数 X_dataset：数据集的实例 batch_size：训练的提取数据的单位内所含个数 shuffle ：布尔值True或者是False ，表示每一个epoch之后是否对样本进行随机打乱，默认是False collate_fn：这个是一个问题，因为通常由自己确定。是一个自己对于数据选取的方法，但是我不知道他有什么意义，或者优势 pin_memory：如果设置为True，那么data loader将会在返回它们之前，将tensors拷贝到CUDA中的固定内存（CUDA pinned memory）中 num_workers：这个参数决定了有几个进程来处理data loading。0意味着所有的数据都会被load进主进程。（默认为0） 1.2 典型的批训练12345678910111213141516171819202122232425262728import torchimport torch.utils.data as Datatorch.manual_seed(1) # reproducibleBATCH_SIZE = 5x = torch.linspace(1, 10, 10) # this is x data (torch tensor)y = torch.linspace(10, 1, 10) # this is y data (torch tensor)torch_dataset = Data.TensorDataset(x, y)loader = Data.DataLoader( dataset=torch_dataset, # torch TensorDataset format batch_size=BATCH_SIZE, # mini batch size shuffle=True, # random shuffle for training num_workers=2, # subprocesses for loading data)def show_batch(): for epoch in range(3): # train entire dataset 3 times for step, (batch_x, batch_y) in enumerate(loader): # for each training step # train your data... print('Epoch: ', epoch, '| Step: ', step, '| batch x: ', batch_x.numpy(), '| batch y: ', batch_y.numpy())if __name__ == '__main__': show_batch() 不难发现，其实批训练的实现就是对Dataset与DataLoader的使用，这里很简单，只是看一个思路。 那么其实每一次训练的时候，对于loader实例的索引，他将带出一组Batch_size个数的数据，但是你的神经网络的输入层的结构不会因为batch_size的变化受到影响 批训练中batch_size对CNN结果的影响，点击这里了解 1.3 collate_fn如何在加载数据时起作用？ DataLoader能够为我们自动生成一个多线程的迭代器，只要传入几个参数进行就可以了,但是你真的懂得它是如何工作的么？ 问题就在这个collate_fn上 collate_fn默认是等于default_collate 1234567def __next__(self): #这是DataLoader的关于迭代的方法的源码 if self.num_workers == 0: # same-process loading indices = next(self.sample_iter) # may raise StopIteration batch = self.collate_fn([self.dataset[i] for i in indices]) # 在这里调用了collate_fn函数，传递的参数是一个列表 if self.pin_memory: batch = pin_memory_batch(batch) return batch 那么其实collate_fn这个函数的输入就是一个list，list的长度是一个batch size，list中的每个元素都是__getitem__得到的结果，想要知道详解点击这里，那么batch中的每一个元素其实就是Dataset中__getitem__方法得到的比如（图片信息img、标签label） 那么如果理解了以上的意思，再看collate_fn就能较快地上手了 那什么时候该使用DataLoader的collate_fn这个参数？ &emsp;&emsp;当定义DataSet类中的__getitem__函数的时候，由于每次返回的是一组类似于（x,y）的样本，但是如果在返回的每一组样本x,y中出现什么错误，或者是还需要进一步对x,y进行一些处理的时候，我们就需要再定义一个collate_fn函数来实现这些功能。当然我也可以自己在实现__getitem__的时候就实现这些后处理也是可以的。 collate_fn，中单词collate的含义是：核对，校勘，对照，整理。顾名思义，这就是一个对每一组样本数据进行一遍“核对和重新整理”，现在可能更好理解一些 2.1 torchvision是什么？ torchvision是PyTorch中专门用来处理图像的库，PyTorch官网的安装教程，也会让你安装上这个包。 这个包中有四个大类 torchvision.datasets torchvision.models torchvision.transforms torchvision.utils torchvision.datasets torchvision.datasets 是用来进行数据加载的，PyTorch团队在这个包中帮我们提前处理好了很多很多图片数据集。 MNISTCOCO Captions Detection LSUN ImageFolder Imagenet-12 CIFAR STL10 SVHN PhotoTour 我们可以直接使用，示例如下： 12345678910111213141516import torchvisiontrainset = torchvison.datasets.MNIST( root = './data', #表示MNIST数据的加载的相对目录 train = True, #表示是否加载数据库的训练集，false的时候加载测试集 download = True， #表示是否自动下载MNIST数据集 transform = None #表示是否需要对数据进行预处理，None为不进行预处理)# 上面代码完成了MNIST数据 训练集的加载环节train_loader2 = Dateloader( dataset = trainset, batch_size = 32, shuffle = True)print(\"训练集总长度为：\",len(trainset))print(\"每个mini-batch的size为32，一共有：\",len(train_loader2),\"个\") torchvision.models torchvision.models中为我们提供了已经训练好的模型，让我们可以加载之后，直接使用 torchvision.models模块的子模块中包含以下模型结构。 AlexNet VGG ResNet SqueezeNet DenseNet 我们可以直接使用如下代码来快速创建一个权重随机初始化的模型 123456import torchvision.models as modelsresnet18 = models.resnet18()alexnet = models.alexnet()squeezenet = models.squeezenet1_0()densenet = models.densenet_161() 也可以通过使用pretrained=True来加载一个别人预训练好的模型 1234import torchvision.models as modelsresnet18 = models.resnet18(pretrained=True)alexnet = models.alexnet(pretrained=True) torchvision.transforms transforms提供了一般的图像转换操作类，这就是我在一开始Dataset中提到了transform，还记得么？哈哈，可以翻一翻上面的例程中在创建Mydataset类的getitem方法中是如何使用transforms的 其实我猜测例程中，作者想在获取每一个图像信息时，除了进行opencv的操作外，训练时还要对图像数据信息做更多处理，而测试时当然就不需要这种处理 12345678910111213141516171819202122# 我们这里还是对MNIST进行处理，初始的MNIST是 28 * 28，我们把它处理成 96 * 96 的torch.Tensor的格式from torchvision import transforms as transformsimport torchvisionfrom torch.utils.data import DataLoader# 图像预处理步骤transform = transforms.Compose([ transforms.Resize(96), # 缩放到 96 * 96 大小 transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 归一化])DOWNLOAD = TrueBATCH_SIZE = 32train_dataset = torchvision.datasets.MNIST(root='./data/', train=True, transform=transform, download=DOWNLOAD)train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)print(len(train_dataset))print(len(train_loader)) 例程中是将从torchvision.datasets中获取的图像数据做transform，这样更简单，那如何在自己定义的Dataset类中加入Transform呢？ 第一步：定义自己的DataSet，并重写__getitem__，在里面实现关键的transform操作 第二步：将多个数据增强方式组合起来合成一个transforms，通过Compose类来实现，注意这个类的返回值哦！ 第三步：构造DataSet的对象，将组合起来的transform传递进去，这样就会对每一个batch_size的图像都进行相关的数据增强操作了 细节问题在于，我没太搞懂这种的数据类型的转换，一会是图片类型，一会是numpy矩阵类型，最后又要转换成tensor张量类型，这个要之后细究 3.1 什么是Resnet？👴也不知道，👴正在学","link":"/2020/04/08/pytorch%E6%B7%B1%E5%85%A5%E4%BA%86%E8%A7%A3/"}],"tags":[{"name":"ROBOCON2020","slug":"ROBOCON2020","link":"/tags/ROBOCON2020/"},{"name":"DT35","slug":"DT35","link":"/tags/DT35/"},{"name":"ielts","slug":"ielts","link":"/tags/ielts/"},{"name":"record","slug":"record","link":"/tags/record/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"PIL","slug":"PIL","link":"/tags/PIL/"},{"name":"adams","slug":"adams","link":"/tags/adams/"},{"name":"markdown","slug":"markdown","link":"/tags/markdown/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"maxwell","slug":"maxwell","link":"/tags/maxwell/"},{"name":"opencv","slug":"opencv","link":"/tags/opencv/"},{"name":"zyc","slug":"zyc","link":"/tags/zyc/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"NNS","slug":"NNS","link":"/tags/NNS/"},{"name":"jupyter","slug":"jupyter","link":"/tags/jupyter/"},{"name":"matlab","slug":"matlab","link":"/tags/matlab/"},{"name":"bluetooth","slug":"bluetooth","link":"/tags/bluetooth/"},{"name":"串口","slug":"串口","link":"/tags/%E4%B8%B2%E5%8F%A3/"},{"name":"定位算法","slug":"定位算法","link":"/tags/%E5%AE%9A%E4%BD%8D%E7%AE%97%E6%B3%95/"},{"name":"CNN","slug":"CNN","link":"/tags/CNN/"}],"categories":[{"name":"ROBOCON2020","slug":"ROBOCON2020","link":"/categories/ROBOCON2020/"},{"name":"Ielts","slug":"Ielts","link":"/categories/Ielts/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"Image Processing","slug":"Image-Processing","link":"/categories/Image-Processing/"},{"name":"hexo","slug":"hexo","link":"/categories/hexo/"},{"name":"Magnetic field specialty","slug":"Magnetic-field-specialty","link":"/categories/Magnetic-field-specialty/"},{"name":"ZYC","slug":"ZYC","link":"/categories/ZYC/"}]}